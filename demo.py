#!/usr/bin/env python3
"""
OpForge å®Œæ•´æ¼”ç¤º

å±•ç¤ºæ‰€æœ‰ä¸»è¦åŠŸèƒ½çš„æ¼”ç¤ºè„šæœ¬ã€‚
"""

import sys
from pathlib import Path

print("ğŸš€ OpForge - æ·±åº¦å­¦ä¹ ç®—å­è‡ªåŠ¨ç”Ÿæˆå·¥å…·")
print("=" * 60)
print()

# æ£€æŸ¥ä¾èµ–
try:
    from opforge.core import Backend, DataType, TensorShape, BackendManager
    from opforge.operators import Conv2DOperator, SoftmaxOperator, MoEOperator
    from opforge.operators.conv_operator import Conv2DConfig
    from opforge.operators.softmax_operator import SoftmaxConfig
    from opforge.operators.moe_operator import MoEConfig
    from opforge.core.code_generator import CodeGenerator
    
    print("âœ… æ‰€æœ‰ä¾èµ–æ¨¡å—å¯¼å…¥æˆåŠŸ")
except ImportError as e:
    print(f"âŒ å¯¼å…¥å¤±è´¥: {e}")
    sys.exit(1)

print()

# 1. æ£€æŸ¥å¯ç”¨åç«¯
print("ğŸ“‹ ç¬¬ä¸€æ­¥ï¼šæ£€æŸ¥å¯ç”¨çš„è®¡ç®—åç«¯")
print("-" * 40)
manager = BackendManager()
backends = manager.get_available_backends()

for backend in backends:
    capability = manager.get_backend_capability(backend)
    print(f"  â€¢ {backend.value}: {capability.name} v{capability.version}")
    
    features = []
    if capability.supports_fp16:
        features.append("FP16")
    if capability.supports_int8:
        features.append("INT8")
    if capability.compute_capability:
        features.append(f"Compute {capability.compute_capability}")
        
    if features:
        print(f"    ç‰¹æ€§: {', '.join(features)}")

optimal_backend = manager.get_optimal_backend()
print(f"\nğŸ¯ æ¨èåç«¯: {optimal_backend.value}")
print()

# 2. æµ‹è¯•Conv2Dç®—å­
print("ğŸ“‹ ç¬¬äºŒæ­¥ï¼šç”Ÿæˆ2Då·ç§¯ç®—å­")
print("-" * 40)

conv_config = Conv2DConfig(
    name="demo_conv2d",
    backend=optimal_backend,
    dtype=DataType.FLOAT32,
    in_channels=64,
    out_channels=128,
    kernel_size=3,
    stride=1,
    padding=1,
    optimization_level=2
)

conv_operator = Conv2DOperator(conv_config)
print(f"âœ… åˆ›å»ºç®—å­: {conv_operator.get_operator_type()}")

# è®¾ç½®è¾“å…¥å½¢çŠ¶
input_shape = TensorShape(
    dims=[8, 64, 224, 224],  # [batch, channels, height, width]
    dtype=DataType.FLOAT32,
    name="input"
)

conv_operator.set_input_shapes([input_shape])
print(f"ğŸ“ è¾“å…¥å½¢çŠ¶: {input_shape}")
print(f"ğŸ“ è¾“å‡ºå½¢çŠ¶: {conv_operator.output_shapes[0]}")

# å†…å­˜åˆ†æ
memory_req = conv_operator.get_memory_requirements()
print(f"ğŸ’¾ å†…å­˜éœ€æ±‚: {memory_req['total_memory_bytes'] / 1024 / 1024:.2f} MB")

# æ€§èƒ½å»ºè®®
hints = conv_operator.get_performance_hints()
if hints:
    print("ğŸ’¡ æ€§èƒ½å»ºè®®:")
    for hint in hints:
        print(f"  â€¢ {hint}")

print()

# 3. æµ‹è¯•Softmaxç®—å­
print("ğŸ“‹ ç¬¬ä¸‰æ­¥ï¼šç”ŸæˆSoftmaxç®—å­")
print("-" * 40)

softmax_config = SoftmaxConfig(
    name="demo_softmax",
    backend=optimal_backend,
    dtype=DataType.FLOAT32,
    dim=-1,
    temperature=1.0,
    use_log_softmax=False,
    optimization_level=2
)

softmax_operator = SoftmaxOperator(softmax_config)
print(f"âœ… åˆ›å»ºç®—å­: {softmax_operator.get_operator_type()}")

# è®¾ç½®è¾“å…¥å½¢çŠ¶
softmax_input_shape = TensorShape(
    dims=[32, 128, 512],  # [batch, seq_len, hidden_dim]
    dtype=DataType.FLOAT32,
    name="attention_scores"
)

softmax_operator.set_input_shapes([softmax_input_shape])
print(f"ğŸ“ è¾“å…¥å½¢çŠ¶: {softmax_input_shape}")
print(f"ğŸ“ è¾“å‡ºå½¢çŠ¶: {softmax_operator.output_shapes[0]}")

print()

# 4. æµ‹è¯•MoEç®—å­
print("ğŸ“‹ ç¬¬å››æ­¥ï¼šç”ŸæˆMoEç®—å­")
print("-" * 40)

moe_config = MoEConfig(
    name="demo_moe",
    backend=optimal_backend,
    dtype=DataType.FLOAT32,
    num_experts=8,
    expert_dim=512,
    hidden_dim=2048,
    top_k=2,
    gate_type="top_k",
    expert_type="ffn",
    optimization_level=2
)

moe_operator = MoEOperator(moe_config)
print(f"âœ… åˆ›å»ºç®—å­: {moe_operator.get_operator_type()}")

# è®¾ç½®è¾“å…¥å½¢çŠ¶
moe_input_shape = TensorShape(
    dims=[16, 64, 512],  # [batch, seq_len, expert_dim]
    dtype=DataType.FLOAT32,
    name="hidden_states"
)

moe_operator.set_input_shapes([moe_input_shape])
print(f"ğŸ“ è¾“å…¥å½¢çŠ¶: {moe_input_shape}")
print(f"ğŸ“ è¾“å‡ºå½¢çŠ¶æ•°é‡: {len(moe_operator.output_shapes)}")

# è®¡ç®—ç¨€ç–æ¯”ç‡
sparsity_ratio = moe_config.top_k / moe_config.num_experts
print(f"âš¡ ç¨€ç–æ¯”ç‡: {sparsity_ratio:.2%} (é€‰æ‹© {moe_config.top_k}/{moe_config.num_experts} ä¸“å®¶)")

print()

# 5. ä»£ç ç”Ÿæˆæ¼”ç¤º
print("ğŸ“‹ ç¬¬äº”æ­¥ï¼šä»£ç ç”Ÿæˆæ¼”ç¤º")
print("-" * 40)

generator = CodeGenerator()

# å°è¯•ç”Ÿæˆä»£ç ï¼ˆå¯èƒ½æ²¡æœ‰æ¨¡æ¿æ–‡ä»¶ï¼‰
operators = [
    ("Conv2D", conv_operator),
    ("Softmax", softmax_operator), 
    ("MoE", moe_operator)
]

for name, operator in operators:
    try:
        # ç”Ÿæˆå‰å‘ä»£ç ï¼ˆç›´æ¥è°ƒç”¨ç®—å­æ–¹æ³•ï¼‰
        forward_code = operator.generate_forward_code()
        if forward_code and len(forward_code) > 50:
            print(f"âœ… {name} å‰å‘ä»£ç ç”ŸæˆæˆåŠŸ ({len(forward_code)} å­—ç¬¦)")
        else:
            print(f"âš ï¸  {name} å‰å‘ä»£ç ç”Ÿæˆä¸ºç©ºæˆ–å¤ªçŸ­")
            
        # ç”Ÿæˆåå‘ä»£ç 
        backward_code = operator.generate_backward_code()
        if backward_code and len(backward_code) > 10:
            print(f"âœ… {name} åå‘ä»£ç ç”ŸæˆæˆåŠŸ ({len(backward_code)} å­—ç¬¦)")
        else:
            print(f"âš ï¸  {name} åå‘ä»£ç ç”Ÿæˆä¸ºç©ºæˆ–å¤ªçŸ­")
            
    except Exception as e:
        print(f"âŒ {name} ä»£ç ç”Ÿæˆå¤±è´¥: {e}")

print()

# 6. æ€»ç»“
print("ğŸ“‹ ç¬¬å…­æ­¥ï¼šæ€»ç»“")
print("-" * 40)

print("ğŸ‰ OpForge æ¼”ç¤ºå®Œæˆï¼")
print()
print("âœ¨ ä¸»è¦åŠŸèƒ½:")
print("  â€¢ âœ… å¤šåç«¯æ”¯æŒ (CPU, CUDA, OpenCL)")
print("  â€¢ âœ… å¤šç§ç®—å­ç±»å‹ (Conv2D, Softmax, MoE)")
print("  â€¢ âœ… è‡ªåŠ¨å½¢çŠ¶æ¨æ–­")
print("  â€¢ âœ… å†…å­˜éœ€æ±‚åˆ†æ")
print("  â€¢ âœ… æ€§èƒ½ä¼˜åŒ–å»ºè®®")
print("  â€¢ âœ… ä»£ç è‡ªåŠ¨ç”Ÿæˆ")
print("  â€¢ âœ… æ¨¡æ¿åŒ–æ¶æ„")
print("  â€¢ âœ… å‘½ä»¤è¡Œå·¥å…·")
print()
print("ğŸš€ ç°åœ¨æ‚¨å¯ä»¥ä½¿ç”¨ OpForge ç”Ÿæˆé«˜æ€§èƒ½çš„æ·±åº¦å­¦ä¹ ç®—å­!")
print()
print("ğŸ“– æ›´å¤šä¿¡æ¯:")
print("  â€¢ æŸ¥çœ‹ docs/USER_GUIDE.md äº†è§£è¯¦ç»†ä½¿ç”¨æ–¹æ³•")
print("  â€¢ è¿è¡Œ examples/ ç›®å½•ä¸‹çš„ç¤ºä¾‹")
print("  â€¢ ä½¿ç”¨ 'opforge --help' æŸ¥çœ‹å‘½ä»¤è¡Œé€‰é¡¹")
print("  â€¢ æŸ¥çœ‹ tests/ ç›®å½•äº†è§£å¦‚ä½•è¿è¡Œæµ‹è¯•")

if __name__ == "__main__":
    pass