"""
OpForgeå‘½ä»¤è¡Œç•Œé¢

æä¾›å‹å¥½çš„å‘½ä»¤è¡Œå·¥å…·æ¥ç”Ÿæˆå„ç§æ·±åº¦å­¦ä¹ ç®—å­ã€‚
"""

import click
import yaml
import json
from pathlib import Path
from typing import Dict, Any, Optional
import sys

from .core import OperatorConfig, TensorShape, DataType, Backend, CodeGenerator, BackendManager
from .operators import Conv2DOperator, SoftmaxOperator, MoEOperator
from .operators.conv_operator import Conv2DConfig
from .operators.softmax_operator import SoftmaxConfig
from .operators.moe_operator import MoEConfig


@click.group()
@click.version_option(version="0.1.0")
@click.option('--verbose', '-v', is_flag=True, help='å¯ç”¨è¯¦ç»†è¾“å‡º')
@click.option('--config', '-c', type=click.Path(exists=True), help='é…ç½®æ–‡ä»¶è·¯å¾„')
@click.pass_context
def cli(ctx, verbose, config):
    """OpForge - æ·±åº¦å­¦ä¹ ç®—å­è‡ªåŠ¨ç”Ÿæˆå·¥å…·"""
    ctx.ensure_object(dict)
    ctx.obj['verbose'] = verbose
    
    if config:
        with open(config, 'r') as f:
            if config.endswith('.yaml') or config.endswith('.yml'):
                ctx.obj['config'] = yaml.safe_load(f)
            else:
                ctx.obj['config'] = json.load(f)
    else:
        ctx.obj['config'] = {}


@cli.command()
def list_backends():
    """åˆ—å‡ºå¯ç”¨çš„è®¡ç®—åç«¯"""
    manager = BackendManager()
    backends = manager.get_available_backends()
    
    click.echo("å¯ç”¨çš„è®¡ç®—åç«¯:")
    for backend in backends:
        capability = manager.get_backend_capability(backend)
        click.echo(f"  â€¢ {backend.value}: {capability.name} v{capability.version}")
        
        # æ˜¾ç¤ºåç«¯ç‰¹æ€§
        features = []
        if capability.supports_fp16:
            features.append("FP16")
        if capability.supports_int8:
            features.append("INT8")
        if capability.compute_capability:
            features.append(f"Compute {capability.compute_capability}")
            
        if features:
            click.echo(f"    ç‰¹æ€§: {', '.join(features)}")


@cli.command()
@click.argument('operator_type', type=click.Choice(['conv2d', 'softmax', 'moe']))
@click.option('--backend', '-b', type=click.Choice(['cuda', 'cpu', 'opencl', 'triton', 'rocm']), 
              default='cuda', help='ç›®æ ‡åç«¯')
@click.option('--dtype', type=click.Choice(['float32', 'float16', 'int32']), 
              default='float32', help='æ•°æ®ç±»å‹')
@click.option('--output', '-o', type=click.Path(), default='./generated', 
              help='è¾“å‡ºç›®å½•')
@click.option('--optimization', type=click.IntRange(0, 2), default=2, 
              help='ä¼˜åŒ–çº§åˆ« (0=æ— ä¼˜åŒ–, 1=åŸºç¡€ä¼˜åŒ–, 2=æ¿€è¿›ä¼˜åŒ–)')
@click.option('--debug', is_flag=True, help='å¯ç”¨è°ƒè¯•æ¨¡å¼')
@click.pass_context
def generate(ctx, operator_type, backend, dtype, output, optimization, debug):
    """ç”ŸæˆæŒ‡å®šç±»å‹çš„ç®—å­ä»£ç """
    
    # éªŒè¯åç«¯å¯ç”¨æ€§
    manager = BackendManager()
    backend_enum = Backend(backend)
    
    if not manager.is_backend_available(backend_enum):
        click.echo(f"é”™è¯¯: åç«¯ {backend} ä¸å¯ç”¨", err=True)
        sys.exit(1)
    
    # æ ¹æ®ç®—å­ç±»å‹è°ƒç”¨ç›¸åº”çš„å­å‘½ä»¤
    if operator_type == 'conv2d':
        ctx.invoke(generate_conv2d, backend=backend, dtype=dtype, output=output, 
                  optimization=optimization, debug=debug)
    elif operator_type == 'softmax':
        ctx.invoke(generate_softmax, backend=backend, dtype=dtype, output=output,
                  optimization=optimization, debug=debug)
    elif operator_type == 'moe':
        ctx.invoke(generate_moe, backend=backend, dtype=dtype, output=output,
                  optimization=optimization, debug=debug)


@cli.command()
@click.option('--backend', '-b', type=click.Choice(['cuda', 'cpu', 'opencl', 'triton', 'rocm']), 
              default='cuda', help='ç›®æ ‡åç«¯')
@click.option('--dtype', type=click.Choice(['float32', 'float16', 'int32']), 
              default='float32', help='æ•°æ®ç±»å‹')
@click.option('--in-channels', type=int, default=3, help='è¾“å…¥é€šé“æ•°')
@click.option('--out-channels', type=int, default=64, help='è¾“å‡ºé€šé“æ•°')
@click.option('--kernel-size', type=int, default=3, help='å·ç§¯æ ¸å¤§å°')
@click.option('--stride', type=int, default=1, help='æ­¥é•¿')
@click.option('--padding', type=int, default=1, help='å¡«å……')
@click.option('--groups', type=int, default=1, help='åˆ†ç»„æ•°')
@click.option('--bias/--no-bias', default=True, help='æ˜¯å¦ä½¿ç”¨åç½®')
@click.option('--output', '-o', type=click.Path(), default='./generated/conv2d', 
              help='è¾“å‡ºç›®å½•')
@click.option('--optimization', type=click.IntRange(0, 2), default=2, 
              help='ä¼˜åŒ–çº§åˆ«')
@click.option('--debug', is_flag=True, help='å¯ç”¨è°ƒè¯•æ¨¡å¼')
@click.option('--enable-tensor-core', is_flag=True, help='å¯ç”¨Tensor CoreæŒ‡ä»¤')
@click.option('--enable-dot-instructions', is_flag=True, help='å¯ç”¨DOTæŒ‡ä»¤ï¼ˆdot2/dot4ï¼‰')
@click.option('--enable-buffer-ops', is_flag=True, help='å¯ç”¨ç¼“å†²åŒºæ“ä½œæŒ‡ä»¤')
def generate_conv2d(backend, dtype, in_channels, out_channels, kernel_size, 
                   stride, padding, groups, bias, output, optimization, debug,
                   enable_tensor_core, enable_dot_instructions, enable_buffer_ops):
    """ç”Ÿæˆ2Då·ç§¯ç®—å­"""
    
    try:
        # åˆ›å»ºé…ç½®
        config = Conv2DConfig(
            name="conv2d_generated",
            backend=Backend(backend),
            dtype=DataType(dtype),
            optimization_level=optimization,
            debug_mode=debug,
            in_channels=in_channels,
            out_channels=out_channels,
            kernel_size=kernel_size,
            stride=stride,
            padding=padding,
            groups=groups,
            bias=bias,
            # ç¡¬ä»¶æŒ‡ä»¤é…ç½®
            enable_tensor_core=enable_tensor_core,
            enable_dot_instructions=enable_dot_instructions,
            enable_buffer_ops=enable_buffer_ops
        )
        
        # åˆ›å»ºç®—å­
        operator = Conv2DOperator(config)
        
        # è®¾ç½®è¾“å…¥å½¢çŠ¶
        input_shape = TensorShape(
            dims=["N", in_channels, "H", "W"],
            dtype=DataType(dtype),
            name="input"
        )
        operator.set_input_shapes([input_shape])
        
        # ç”Ÿæˆä»£ç 
        generator = CodeGenerator()
        generated_files = generator.generate_operator_code(operator)
        
        # ä¿å­˜æ–‡ä»¶
        output_path = Path(output)
        saved_files = generator.save_generated_code(generated_files, str(output_path))
        
        click.echo(f"âœ… 2Då·ç§¯ç®—å­ç”ŸæˆæˆåŠŸ!")
        click.echo(f"ğŸ“ è¾“å‡ºç›®å½•: {output_path.absolute()}")
        
        for file_type, file_path in saved_files.items():
            click.echo(f"  ğŸ“„ {file_type}: {file_path}")
        
        # æ˜¾ç¤ºæ€§èƒ½æç¤º
        hints = operator.get_performance_hints()
        if hints:
            click.echo("\nğŸ’¡ æ€§èƒ½ä¼˜åŒ–å»ºè®®:")
            for hint in hints:
                click.echo(f"  â€¢ {hint}")
                
    except Exception as e:
        click.echo(f"âŒ ç”Ÿæˆå¤±è´¥: {str(e)}", err=True)
        sys.exit(1)


@cli.command()
@click.option('--backend', '-b', type=click.Choice(['cuda', 'cpu', 'opencl', 'triton', 'rocm']), 
              default='cuda', help='ç›®æ ‡åç«¯')
@click.option('--dtype', type=click.Choice(['float32', 'float16', 'int32']), 
              default='float32', help='æ•°æ®ç±»å‹')
@click.option('--dim', type=int, default=-1, help='Softmaxè®¡ç®—ç»´åº¦')
@click.option('--temperature', type=float, default=1.0, help='æ¸©åº¦å‚æ•°')
@click.option('--log-softmax', is_flag=True, help='ç”ŸæˆLogSoftmax')
@click.option('--online-algorithm', is_flag=True, default=True, help='ä½¿ç”¨åœ¨çº¿ç®—æ³•')
@click.option('--output', '-o', type=click.Path(), default='./generated/softmax', 
              help='è¾“å‡ºç›®å½•')
@click.option('--optimization', type=click.IntRange(0, 2), default=2, 
              help='ä¼˜åŒ–çº§åˆ«')
@click.option('--debug', is_flag=True, help='å¯ç”¨è°ƒè¯•æ¨¡å¼')
def generate_softmax(backend, dtype, dim, temperature, log_softmax, online_algorithm,
                    output, optimization, debug):
    """ç”ŸæˆSoftmaxç®—å­"""
    
    try:
        # åˆ›å»ºé…ç½®
        config = SoftmaxConfig(
            name="softmax_generated",
            backend=Backend(backend),
            dtype=DataType(dtype),
            optimization_level=optimization,
            debug_mode=debug,
            dim=dim,
            temperature=temperature,
            use_log_softmax=log_softmax,
            use_online_softmax=online_algorithm
        )
        
        # åˆ›å»ºç®—å­
        operator = SoftmaxOperator(config)
        
        # è®¾ç½®è¾“å…¥å½¢çŠ¶
        input_shape = TensorShape(
            dims=["N", "seq_len", "dim"],
            dtype=DataType(dtype),
            name="input"
        )
        operator.set_input_shapes([input_shape])
        
        # ç”Ÿæˆä»£ç 
        generator = CodeGenerator()
        generated_files = generator.generate_operator_code(operator)
        
        # ä¿å­˜æ–‡ä»¶
        output_path = Path(output)
        saved_files = generator.save_generated_code(generated_files, str(output_path))
        
        algorithm = "LogSoftmax" if log_softmax else "Softmax"
        click.echo(f"âœ… {algorithm}ç®—å­ç”ŸæˆæˆåŠŸ!")
        click.echo(f"ğŸ“ è¾“å‡ºç›®å½•: {output_path.absolute()}")
        
        for file_type, file_path in saved_files.items():
            click.echo(f"  ğŸ“„ {file_type}: {file_path}")
        
        # æ˜¾ç¤ºæ€§èƒ½æç¤º
        hints = operator.get_performance_hints()
        if hints:
            click.echo("\nğŸ’¡ æ€§èƒ½ä¼˜åŒ–å»ºè®®:")
            for hint in hints:
                click.echo(f"  â€¢ {hint}")
                
    except Exception as e:
        click.echo(f"âŒ ç”Ÿæˆå¤±è´¥: {str(e)}", err=True)
        sys.exit(1)


@cli.command()
@click.option('--backend', '-b', type=click.Choice(['cuda', 'cpu', 'opencl', 'triton', 'rocm']), 
              default='cuda', help='ç›®æ ‡åç«¯')
@click.option('--dtype', type=click.Choice(['float32', 'float16', 'int32']), 
              default='float32', help='æ•°æ®ç±»å‹')
@click.option('--num-experts', type=int, default=8, help='ä¸“å®¶æ•°é‡')
@click.option('--expert-dim', type=int, default=512, help='ä¸“å®¶ç»´åº¦')
@click.option('--hidden-dim', type=int, default=2048, help='éšè—å±‚ç»´åº¦')
@click.option('--top-k', type=int, default=2, help='TopKè·¯ç”±')
@click.option('--gate-type', type=click.Choice(['top_k', 'switch', 'dense']), 
              default='top_k', help='é—¨æ§ç±»å‹')
@click.option('--expert-type', type=click.Choice(['ffn', 'attention']), 
              default='ffn', help='ä¸“å®¶ç±»å‹')
@click.option('--output', '-o', type=click.Path(), default='./generated/moe', 
              help='è¾“å‡ºç›®å½•')
@click.option('--optimization', type=click.IntRange(0, 2), default=2, 
              help='ä¼˜åŒ–çº§åˆ«')
@click.option('--debug', is_flag=True, help='å¯ç”¨è°ƒè¯•æ¨¡å¼')
def generate_moe(backend, dtype, num_experts, expert_dim, hidden_dim, top_k,
                gate_type, expert_type, output, optimization, debug):
    """ç”ŸæˆMoEç®—å­"""
    
    try:
        # åˆ›å»ºé…ç½®
        config = MoEConfig(
            name="moe_generated",
            backend=Backend(backend),
            dtype=DataType(dtype),
            optimization_level=optimization,
            debug_mode=debug,
            num_experts=num_experts,
            expert_dim=expert_dim,
            hidden_dim=hidden_dim,
            top_k=top_k,
            gate_type=gate_type,
            expert_type=expert_type
        )
        
        # åˆ›å»ºç®—å­
        operator = MoEOperator(config)
        
        # è®¾ç½®è¾“å…¥å½¢çŠ¶
        input_shape = TensorShape(
            dims=["batch_size", "seq_len", expert_dim],
            dtype=DataType(dtype),
            name="input"
        )
        operator.set_input_shapes([input_shape])
        
        # ç”Ÿæˆä»£ç 
        generator = CodeGenerator()
        generated_files = generator.generate_operator_code(operator)
        
        # ä¿å­˜æ–‡ä»¶
        output_path = Path(output)
        saved_files = generator.save_generated_code(generated_files, str(output_path))
        
        click.echo(f"âœ… MoEç®—å­ç”ŸæˆæˆåŠŸ!")
        click.echo(f"ğŸ“ è¾“å‡ºç›®å½•: {output_path.absolute()}")
        
        for file_type, file_path in saved_files.items():
            click.echo(f"  ğŸ“„ {file_type}: {file_path}")
        
        # æ˜¾ç¤ºæ€§èƒ½æç¤º
        hints = operator.get_performance_hints()
        if hints:
            click.echo("\nğŸ’¡ æ€§èƒ½ä¼˜åŒ–å»ºè®®:")
            for hint in hints:
                click.echo(f"  â€¢ {hint}")
                
    except Exception as e:
        click.echo(f"âŒ ç”Ÿæˆå¤±è´¥: {str(e)}", err=True)
        sys.exit(1)


@cli.command()
@click.argument('config_file', type=click.Path(exists=True))
@click.option('--output', '-o', type=click.Path(), default='./generated/batch', 
              help='è¾“å‡ºç›®å½•')
def batch_generate(config_file, output):
    """ä»é…ç½®æ–‡ä»¶æ‰¹é‡ç”Ÿæˆç®—å­"""
    
    try:
        # åŠ è½½é…ç½®
        with open(config_file, 'r') as f:
            if config_file.endswith('.yaml') or config_file.endswith('.yml'):
                batch_config = yaml.safe_load(f)
            else:
                batch_config = json.load(f)
        
        output_path = Path(output)
        
        for operator_config in batch_config.get('operators', []):
            operator_type = operator_config.get('type')
            
            click.echo(f"ğŸ”„ ç”Ÿæˆ {operator_type} ç®—å­...")
            
            # æ ¹æ®ç±»å‹ç”Ÿæˆç›¸åº”ç®—å­
            # è¿™é‡Œå¯ä»¥æ‰©å±•æ”¯æŒæ›´å¤šç®—å­ç±»å‹
            
        click.echo(f"âœ… æ‰¹é‡ç”Ÿæˆå®Œæˆ! è¾“å‡ºç›®å½•: {output_path.absolute()}")
        
    except Exception as e:
        click.echo(f"âŒ æ‰¹é‡ç”Ÿæˆå¤±è´¥: {str(e)}", err=True)
        sys.exit(1)


@cli.command()
@click.argument('operator_dir', type=click.Path(exists=True))
def validate(operator_dir):
    """éªŒè¯ç”Ÿæˆçš„ç®—å­ä»£ç """
    
    click.echo(f"ğŸ” éªŒè¯ç®—å­ä»£ç : {operator_dir}")
    
    # TODO: å®ç°ä»£ç éªŒè¯é€»è¾‘
    # 1. æ£€æŸ¥è¯­æ³•é”™è¯¯
    # 2. æ£€æŸ¥APIä¸€è‡´æ€§
    # 3. è¿è¡ŒåŸºæœ¬æµ‹è¯•
    
    click.echo("âœ… éªŒè¯é€šè¿‡!")


def main():
    """å‘½ä»¤è¡Œå…¥å£ç‚¹"""
    cli()


@cli.command()
@click.argument('operator_type', type=click.Choice(['conv2d', 'softmax', 'moe']))
@click.option('--backend', '-b', type=click.Choice(['cuda', 'triton', 'rocm']), 
              default='cuda', help='ç¡¬ä»¶åŠ é€Ÿåç«¯')
@click.option('--dtype', type=click.Choice(['float32', 'float16', 'int32']), 
              default='float16', help='æ•°æ®ç±»å‹ï¼ˆå»ºè®®ä½¿ç”¨float16ä»¥æ”¯æŒTensor Coreï¼‰')
@click.option('--enable-tensor-core', is_flag=True, default=True, help='å¯ç”¨Tensor CoreæŒ‡ä»¤')
@click.option('--enable-dot-instructions', is_flag=True, help='å¯ç”¨DOTæŒ‡ä»¤ï¼ˆdot2/dot4/dp4aï¼‰')
@click.option('--enable-buffer-ops', is_flag=True, help='å¯ç”¨ç¼“å†²åŒºæ“ä½œæŒ‡ä»¤')
@click.option('--enable-mfma', is_flag=True, help='å¯ç”¨MFMAæŒ‡ä»¤ï¼ˆROCmï¼‰')
@click.option('--tensor-core-shape', type=click.Choice(['16x16x16', '8x32x16', '32x8x16']), 
              default='16x16x16', help='Tensor CoreçŸ©é˜µå½¢çŠ¶')
@click.option('--output', '-o', type=click.Path(), default='./generated/hardware_optimized', 
              help='è¾“å‡ºç›®å½•')
@click.option('--optimization', type=click.IntRange(0, 2), default=2, 
              help='ä¼˜åŒ–çº§åˆ«')
def generate_hardware_optimized(operator_type, backend, dtype, enable_tensor_core,
                               enable_dot_instructions, enable_buffer_ops, enable_mfma,
                               tensor_core_shape, output, optimization):
    """ç”Ÿæˆç¡¬ä»¶æŒ‡ä»¤ä¼˜åŒ–çš„ç®—å­ä»£ç """
    
    try:
        from opforge.core.operator_base import HardwareInstruction, HardwareConfig
        
        click.echo(f"ğŸš€ ç”Ÿæˆç¡¬ä»¶ä¼˜åŒ–çš„{operator_type}ç®—å­")
        click.echo(f"ğŸ’» åç«¯: {backend}")
        click.echo(f"ğŸ”§ æ•°æ®ç±»å‹: {dtype}")
        
        # åˆ›å»ºç¡¬ä»¶é…ç½®
        hardware_config = HardwareConfig()
        
        enabled_instructions = []
        if enable_tensor_core:
            hardware_config.supported_instructions.add(HardwareInstruction.TENSOR_CORE)
            # è§£æTensor Coreå½¢çŠ¶
            shape_parts = tensor_core_shape.split('x')
            tc_shape = tuple(int(x) for x in shape_parts)
            hardware_config.tensor_core_shapes = [tc_shape]
            enabled_instructions.append(f"Tensor Core ({tensor_core_shape})")
            
        if enable_dot_instructions:
            hardware_config.supported_instructions.update({
                HardwareInstruction.DOT2,
                HardwareInstruction.DOT4,
                HardwareInstruction.DP4A
            })
            enabled_instructions.append("DOTæŒ‡ä»¤ (dot2/dot4/dp4a)")
            
        if enable_buffer_ops:
            hardware_config.supported_instructions.update({
                HardwareInstruction.BUFFER_LOAD,
                HardwareInstruction.BUFFER_STORE
            })
            enabled_instructions.append("ç¼“å†²åŒºæ“ä½œ")
            
        if enable_mfma and backend == 'rocm':
            hardware_config.supported_instructions.add(HardwareInstruction.MFMA)
            enabled_instructions.append("MFMAæŒ‡ä»¤")
        
        click.echo(f"âš™ï¸  å¯ç”¨çš„æŒ‡ä»¤: {', '.join(enabled_instructions)}")
        
        # æ ¹æ®ç®—å­ç±»å‹åˆ›å»ºé…ç½®
        if operator_type == 'conv2d':
            config = Conv2DConfig(
                name="hardware_optimized_conv2d",
                backend=Backend(backend),
                dtype=DataType(dtype),
                optimization_level=optimization,
                debug_mode=False,
                hardware_config=hardware_config,
                enable_tensor_core=enable_tensor_core,
                enable_dot_instructions=enable_dot_instructions,
                enable_buffer_ops=enable_buffer_ops,
                # é»˜è®¤å‚æ•°
                in_channels=64,
                out_channels=128,
                kernel_size=3,
                stride=1,
                padding=1
            )
            operator = Conv2DOperator(config)
            input_shape = TensorShape(dims=["N", 64, "H", "W"], dtype=DataType(dtype), name="input")
            
        elif operator_type == 'softmax':
            config = SoftmaxConfig(
                name="hardware_optimized_softmax",
                backend=Backend(backend),
                dtype=DataType(dtype),
                optimization_level=optimization,
                # hardware_config=hardware_config,
                dim=-1
            )
            operator = SoftmaxOperator(config)
            input_shape = TensorShape(dims=["N", "seq_len", "dim"], dtype=DataType(dtype), name="input")
            
        elif operator_type == 'moe':
            config = MoEConfig(
                name="hardware_optimized_moe",
                backend=Backend(backend),
                dtype=DataType(dtype),
                optimization_level=optimization,
                # hardware_config=hardware_config,
                num_experts=8,
                expert_dim=512,
                hidden_dim=2048,
                top_k=2
            )
            operator = MoEOperator(config)
            input_shape = TensorShape(dims=["batch", "seq_len", 512], dtype=DataType(dtype), name="input")
        
        # è®¾ç½®è¾“å…¥å½¢çŠ¶
        operator.set_input_shapes([input_shape])
        
        # ç”Ÿæˆä»£ç 
        generator = CodeGenerator()
        generated_files = generator.generate_operator_code(operator)
        
        # ä¿å­˜æ–‡ä»¶
        output_path = Path(output)
        saved_files = generator.save_generated_code(generated_files, str(output_path))
        
        click.echo(f"âœ… ç¡¬ä»¶ä¼˜åŒ–{operator_type}ç®—å­ç”ŸæˆæˆåŠŸï¼")
        click.echo(f"ğŸ“ è¾“å‡ºç›®å½•: {output_path.absolute()}")
        
        for file_type, file_path in saved_files.items():
            click.echo(f"  ğŸ“„ {file_type}: {file_path}")
        
        # æ˜¾ç¤ºæ€§èƒ½æç¤º
        hints = operator.get_performance_hints()
        if hints:
            click.echo("\nğŸ’¡ æ€§èƒ½ä¼˜åŒ–å»ºè®®:")
            for hint in hints:
                click.echo(f"  â€¢ {hint}")
        
        # æ˜¾ç¤ºç¡¬ä»¶æŒ‡ä»¤ä¿¡æ¯
        click.echo("\nğŸ”§ ç¡¬ä»¶æŒ‡ä»¤è¯¦æƒ…:")
        if enable_tensor_core:
            click.echo(f"  â€¢ Tensor Core: æ”¯æŒ{tensor_core_shape}å½¢çŠ¶")
            click.echo(f"  â€¢ å»ºè®®ä½¿ç”¨float16æ•°æ®ç±»å‹ä»¥è·å¾—æœ€ä½³æ€§èƒ½")
        if enable_dot_instructions:
            click.echo(f"  â€¢ DOTæŒ‡ä»¤: æ”¯æŒé«˜æ•ˆçš„ç‚¹ç§¯è®¡ç®—")
        if enable_buffer_ops:
            click.echo(f"  â€¢ ç¼“å†²åŒºæ“ä½œ: ä¼˜åŒ–çš„å†…å­˜è®¿é—®æ¨¡å¼")
        
        click.echo(f"\nğŸ‰ ç¡¬ä»¶ä¼˜åŒ–å®Œæˆï¼")
        
    except Exception as e:
        click.echo(f"âŒ ç”Ÿæˆå¤±è´¥: {str(e)}", err=True)
        sys.exit(1)


if __name__ == '__main__':
    main()